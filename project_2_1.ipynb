{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Finding Co-occurrence Patterns of Points of Interest (POI)\n",
    "\n",
    "Co-occurence: Occurence of different combinations of POI at the same time \n",
    "\n",
    "Idea: \n",
    "\n",
    "Given that we have Grid Cells (x,y) which denotes **what** and **how many** POI are in that grid, we can mine co-occurence that means: \n",
    "\n",
    "\"What is the common occurance of **different** POIs that would likely appear in one grid\"\n",
    "\n",
    "\n",
    "### Illustrating with an example: \n",
    "\n",
    "Grid cell (x1, y1) has categories: restaurant, park, gym\n",
    "\n",
    "Grid cell (x2, y2) has categories: restaurant, park\n",
    "\n",
    "Grid cell (x3, y3) has categories: restaurant, cafe\n",
    "\n",
    "\n",
    "Running Apriori with a minimum support threshold of 2 (meaning the combination should appear in at least 2 grid cells), \n",
    "\n",
    "Frequent itemset = {restaurant, park} because restaurant and park appear together in two grid cells ((x1, y1) and (x2, y2)).\n",
    "\n",
    "\n",
    "### Terminologies \n",
    "\n",
    "- Transaction / Baskets = An entry in the database\n",
    "\n",
    "- Itemset I = {Set of items that appear together}\n",
    "\n",
    "- Support of Item set I = Number of transactions that contains Itemset I \n",
    "\n",
    "- Frequent Itemset = | I | >= minsup \n",
    "\n",
    "From frequent itemsets {A,B,C,D}, we can generate {association rules}: \n",
    "\n",
    "- Confidence of an association rule: P({current items} -> Item j) \n",
    "\n",
    "\n",
    "### Determining Frequent Itemsets \n",
    "```pseudo\n",
    "if support(I) >= min_sup: \n",
    "    I = frequent_itemset: \n",
    "else:\n",
    "    not frquenet \n",
    "\n",
    "```\n",
    "### Determining Association Rules  \n",
    "\n",
    "```pseudo\n",
    "for all elements in frequent itemset {A, B...}:\n",
    "    generate A->B \n",
    "\n",
    "    conf(A->B) = P(AB)/P(A) \n",
    "\n",
    "    if conf(A->B) > min_conf: \n",
    "        A->B = Association Rule \n",
    "\n",
    "    repeat for different implications (A->C, A->D... AB->C...) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformating CSV \n",
    "\n",
    "- Treat each unique grid as a transaction / basket \n",
    "\n",
    "- For POIs in that unique grid, string all of them together => This becomes a unique transaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(input_file, output_file):\n",
    "\n",
    "    data = pd.read_csv(input_file)\n",
    "    grouped = data.groupby(['x', 'y'])['category'].apply(list).reset_index()\n",
    "    max_pois = grouped['category'].apply(len).max()\n",
    "    poi_columns = grouped['category'].apply(lambda pois: [str(poi) for poi in pois] + [None]*(max_pois - len(pois)))\n",
    "    poi_df = pd.DataFrame(poi_columns.tolist(), columns=[f'poi{i+1}' for i in range(max_pois)])\n",
    "    result = pd.concat([grouped[['x', 'y']], poi_df], axis=1)\n",
    "    result.to_csv(output_file, index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_subsets(item_list):\n",
    "    \"\"\"Returns non-empty subsets of item_list.\"\"\"\n",
    "    return chain(*[combinations(item_list, i + 1) for i, item in enumerate(item_list)])\n",
    "\n",
    "def filter_items_by_support(candidate_itemsets, transactions, min_support, item_support_counts):\n",
    "    \"\"\"Calculates the support for items in candidate_itemsets and returns a subset\n",
    "    that meets the minimum support threshold.\"\"\"\n",
    "    qualified_itemsets = set()\n",
    "    local_item_counts = defaultdict(int)\n",
    "\n",
    "    for itemset in candidate_itemsets:\n",
    "        for transaction in transactions:\n",
    "            if itemset.issubset(transaction):\n",
    "                item_support_counts[itemset] += 1\n",
    "                local_item_counts[itemset] += 1\n",
    "\n",
    "    for itemset, count in local_item_counts.items():\n",
    "        support = float(count) / len(transactions)\n",
    "        if support >= min_support:\n",
    "            qualified_itemsets.add(itemset)\n",
    "\n",
    "    return qualified_itemsets\n",
    "\n",
    "def merge_itemsets(itemsets, length):\n",
    "    \"\"\"Joins a set with itself and returns the merged n-element itemsets.\"\"\"\n",
    "    return set(\n",
    "        [i.union(j) for i in itemsets for j in itemsets if len(i.union(j)) == length]\n",
    "    )\n",
    "\n",
    "def get_initial_itemsets_and_transactions(data_iterator):\n",
    "    transactions = []\n",
    "    initial_itemsets = set()\n",
    "    for record in data_iterator:\n",
    "        transaction = frozenset(record)\n",
    "        transactions.append(transaction)\n",
    "        for item in transaction:\n",
    "            initial_itemsets.add(frozenset([item]))  # Generate 1-itemsets\n",
    "    return initial_itemsets, transactions\n",
    "\n",
    "def apriori_algorithm(data_iter, min_support, min_confidence):\n",
    "    \"\"\"\n",
    "    Executes the Apriori algorithm. data_iter is a record iterator.\n",
    "    Returns:\n",
    "     - frequent_itemsets (tuple, support)\n",
    "     - association_rules ((antecedent, consequent), confidence)\n",
    "    \"\"\"\n",
    "    initial_itemsets, transactions = get_initial_itemsets_and_transactions(data_iter)\n",
    "\n",
    "    item_support_counts = defaultdict(int)\n",
    "    frequent_itemsets_by_length = dict()\n",
    "    association_rules = dict()\n",
    "\n",
    "    one_itemsets = filter_items_by_support(initial_itemsets, transactions, min_support, item_support_counts)\n",
    "\n",
    "    current_itemsets = one_itemsets\n",
    "    k = 2\n",
    "    while current_itemsets:\n",
    "        frequent_itemsets_by_length[k - 1] = current_itemsets\n",
    "        current_itemsets = merge_itemsets(current_itemsets, k)\n",
    "        candidate_itemsets = filter_items_by_support(\n",
    "            current_itemsets, transactions, min_support, item_support_counts\n",
    "        )\n",
    "        current_itemsets = candidate_itemsets\n",
    "        k += 1\n",
    "\n",
    "    def calculate_support(itemset):\n",
    "        \"\"\"Local function to return the support of an itemset.\"\"\"\n",
    "        return float(item_support_counts[itemset]) / len(transactions)\n",
    "\n",
    "    frequent_itemsets = []\n",
    "    for itemset_length, itemsets in frequent_itemsets_by_length.items():\n",
    "        frequent_itemsets.extend([(tuple(itemset), calculate_support(itemset)) for itemset in itemsets])\n",
    "\n",
    "    association_rules = []\n",
    "    for itemset_length, itemsets in list(frequent_itemsets_by_length.items())[1:]:\n",
    "        for itemset in itemsets:\n",
    "            itemset_subsets = map(frozenset, [x for x in generate_subsets(itemset)])\n",
    "            for subset in itemset_subsets:\n",
    "                remaining_items = itemset.difference(subset)\n",
    "                if remaining_items:\n",
    "                    confidence = calculate_support(itemset) / calculate_support(subset)\n",
    "                    if confidence >= min_confidence:\n",
    "                        association_rules.append(((tuple(subset), tuple(remaining_items)), confidence))\n",
    "    return frequent_itemsets, association_rules\n",
    "\n",
    "def display_results(frequent_itemsets, association_rules):\n",
    "    \"\"\"Displays the frequent itemsets sorted by support and the association rules sorted by confidence.\"\"\"\n",
    "    for itemset, support in sorted(frequent_itemsets, key=lambda x: x[1]):\n",
    "        print(\"Itemset: %s , %.3f\" % (str(itemset), support))\n",
    "    print(\"\\n------------------------ ASSOCIATION RULES:\")\n",
    "    for rule, confidence in sorted(association_rules, key=lambda x: x[1]):\n",
    "        antecedent, consequent = rule\n",
    "        print(\"Rule: %s ==> %s , %.3f\" % (str(antecedent), str(consequent), confidence))\n",
    "\n",
    "def format_results_to_strings(frequent_itemsets, association_rules):\n",
    "    \"\"\"Formats the frequent itemsets and association rules into strings sorted by support and confidence.\"\"\"\n",
    "    itemset_strings, rule_strings = [], []\n",
    "    for itemset, support in sorted(frequent_itemsets, key=lambda x: x[1]):\n",
    "        itemset_str = \"Itemset: %s , %.3f\" % (str(itemset), support)\n",
    "        itemset_strings.append(itemset_str)\n",
    "\n",
    "    for rule, confidence in sorted(association_rules, key=lambda x: x[1]):\n",
    "        antecedent, consequent = rule\n",
    "        rule_str = \"Rule: %s ==> %s , %.3f\" % (str(antecedent), str(consequent), confidence)\n",
    "        rule_strings.append(rule_str)\n",
    "\n",
    "    return itemset_strings, rule_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the above code requires a csv in the form of \n",
    "\n",
    "Transaction 1 items\n",
    "\n",
    "Transaction 2 items \n",
    "... \n",
    "\n",
    "Modify the csv to appear as such "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_apriori(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        next(infile)\n",
    "        \n",
    "        for line in infile:\n",
    "            parts = line.strip().split(',')\n",
    "        \n",
    "            pois = [poi.strip('\"') for poi in parts[2:] if poi]  \n",
    "            pois_line = ','.join(pois)\n",
    "            outfile.write(f\"{pois_line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataFromFile(fname):\n",
    "    \"\"\"Function which reads from the file and yields a generator\"\"\"\n",
    "    with open(fname, \"r\") as file_iter: \n",
    "        for line in file_iter:\n",
    "            line = line.strip().rstrip(\",\")  \n",
    "            record = frozenset(line.split(\",\"))  \n",
    "            yield record\n",
    "\n",
    "def run_apriori_and_save_results(input_file, min_support, min_confidence, dataset_label):\n",
    "    inFile = dataFromFile(input_file)\n",
    "    items, rules = apriori_algorithm(inFile, min_support, min_confidence)\n",
    "    \n",
    "\n",
    "    with open(f'output/itemsets_{dataset_label}.csv', 'w', newline='') as items_file:\n",
    "        writer = csv.writer(items_file)\n",
    "        writer.writerow(['Itemset', 'Support'])\n",
    "        for item, support in items:\n",
    "            writer.writerow([', '.join(item), support])\n",
    "\n",
    "    with open(f'output/rules_{dataset_label}.csv', 'w', newline='') as rules_file:\n",
    "        writer = csv.writer(rules_file)\n",
    "        writer.writerow(['Rule', 'Confidence'])\n",
    "        for (pre, post), confidence in rules:\n",
    "            rule = f\"{', '.join(pre)} => {', '.join(post)}\"\n",
    "            writer.writerow([rule, confidence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_files, min_support, min_confidence):\n",
    "    for input_file in input_files:\n",
    "        dataset_label = input_file.split('_')[-1].replace('.csv', '')  # Extract 'A', 'B', 'C', or 'D'\n",
    "        \n",
    "        transformed_file = f'data/transformed_{dataset_label}.csv'\n",
    "        preprocess_data(input_file, transformed_file)\n",
    "        \n",
    "        prepared_file = f'data/prepared_data_{dataset_label}.csv'\n",
    "        prepare_data_for_apriori(transformed_file, prepared_file)\n",
    "        \n",
    "        run_apriori_and_save_results(prepared_file, min_support, min_confidence, dataset_label)\n",
    "\n",
    "input_files = [\n",
    "    'data/POIdata_cityA.csv',\n",
    "    'data/POIdata_cityB.csv',\n",
    "    'data/POIdata_cityC.csv',\n",
    "    'data/POIdata_cityD.csv'\n",
    "]\n",
    "min_support = 0.10\n",
    "min_confidence = 0.6\n",
    "\n",
    "main(input_files, min_support, min_confidence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the rules mined,\n",
    "\n",
    "observe that City B, D have about 40 Rules, while A, C have about 20k Rules. \n",
    "\n",
    "In order to pick the best assoication rules, select the top 𝐾 association rules based on confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top 10 rules for output/rules_cityA.csv to top_output\\top_rulescityA.csv\n",
      "Saved top 10 rules for output/rules_cityB.csv to top_output\\top_rulescityB.csv\n",
      "Saved top 10 rules for output/rules_cityC.csv to top_output\\top_rulescityC.csv\n",
      "Saved top 10 rules for output/rules_cityD.csv to top_output\\top_rulescityD.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_files = [\n",
    "    'output/rules_cityA.csv',\n",
    "    'output/rules_cityB.csv',\n",
    "    'output/rules_cityC.csv',\n",
    "    'output/rules_cityD.csv'\n",
    "]\n",
    "\n",
    "output_dir = 'top_output'\n",
    "os.makedirs(output_dir, exist_ok=True) \n",
    "\n",
    "for file_path in input_files:\n",
    "   \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    top_k_rules = df.sort_values(by=\"Confidence\", ascending=False).head(20)\n",
    "    \n",
    "    city_name = os.path.basename(file_path).replace(\"rules_\", \"top_rules\")  # Replace 'rules_' with 'top_10_'\n",
    "    output_file = os.path.join(output_dir, city_name)\n",
    "\n",
    "    top_k_rules.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Saved top 10 rules for {file_path} to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
