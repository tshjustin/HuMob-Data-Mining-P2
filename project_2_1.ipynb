{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Finding Co-occurrence Patterns of Points of Interest (POI)\n",
    "\n",
    "Co-occurence: Occurence of different combinations of POI at the same time \n",
    "\n",
    "Idea: \n",
    "\n",
    "Given that we have Grid Cells (x,y) which denotes **what** and **how many** POI are in that grid, we can mine co-occurence that means: \n",
    "\n",
    "\"What is the common occurance of **different** POIs that would likely appear in one grid\"\n",
    "\n",
    "\n",
    "### Illustrating with an example: \n",
    "\n",
    "Grid cell (x1, y1) has categories: restaurant, park, gym\n",
    "\n",
    "Grid cell (x2, y2) has categories: restaurant, park\n",
    "\n",
    "Grid cell (x3, y3) has categories: restaurant, cafe\n",
    "\n",
    "\n",
    "Running Apriori with a minimum support threshold of 2 (meaning the combination should appear in at least 2 grid cells), \n",
    "\n",
    "Frequent itemset = {restaurant, park} because restaurant and park appear together in two grid cells ((x1, y1) and (x2, y2)).\n",
    "\n",
    "\n",
    "### Terminologies \n",
    "\n",
    "- Transaction / Baskets = An entry in the database\n",
    "\n",
    "- Itemset I = {Set of items that appear together}\n",
    "\n",
    "- Support of Item set I = Number of transactions that contains Itemset I \n",
    "\n",
    "- Frequent Itemset = | I | >= minsup \n",
    "\n",
    "From frequent itemsets {A,B,C,D}, we can generate {association rules}: \n",
    "\n",
    "- Confidence of an association rule: P({current items} -> Item j) \n",
    "\n",
    "\n",
    "### Determining Frequent Itemsets \n",
    "```pseudo\n",
    "if support(I) >= min_sup: \n",
    "    I = frequent_itemset: \n",
    "else:\n",
    "    not frquenet \n",
    "\n",
    "```\n",
    "### Determining Association Rules  \n",
    "\n",
    "```pseudo\n",
    "for all elements in frequent itemset {A, B...}:\n",
    "    generate A->B \n",
    "\n",
    "    conf(A->B) = P(AB)/P(A) \n",
    "\n",
    "    if conf(A->B) > min_conf: \n",
    "        A->B = Association Rule \n",
    "\n",
    "    repeat for different implications (A->C, A->D... AB->C...) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "from optparse import OptionParser\n",
    "import pandas as pd\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformating CSV \n",
    "\n",
    "- Treat each unique grid as a transaction / basket \n",
    "\n",
    "- For POIs in that unique grid, string all of them together => This becomes a unique transaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(input_file, output_file):\n",
    "\n",
    "    data = pd.read_csv(input_file)\n",
    "    grouped = data.groupby(['x', 'y'])['category'].apply(list).reset_index()\n",
    "    max_pois = grouped['category'].apply(len).max()\n",
    "    poi_columns = grouped['category'].apply(lambda pois: [str(poi) for poi in pois] + [None]*(max_pois - len(pois)))\n",
    "    poi_df = pd.DataFrame(poi_columns.tolist(), columns=[f'poi{i+1}' for i in range(max_pois)])\n",
    "    result = pd.concat([grouped[['x', 'y']], poi_df], axis=1)\n",
    "    result.to_csv(output_file, index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsets(arr):\n",
    "    \"\"\" Returns non empty subsets of arr\"\"\"\n",
    "    return chain(*[combinations(arr, i + 1) for i, a in enumerate(arr)])\n",
    "\n",
    "\n",
    "def returnItemsWithMinSupport(itemSet, transactionList, minSupport, freqSet):\n",
    "    \"\"\"calculates the support for items in the itemSet and returns a subset\n",
    "    of the itemSet each of whose elements satisfies the minimum support\"\"\"\n",
    "    _itemSet = set()\n",
    "    localSet = defaultdict(int)\n",
    "\n",
    "    for item in itemSet:\n",
    "        for transaction in transactionList:\n",
    "            if item.issubset(transaction):\n",
    "                freqSet[item] += 1\n",
    "                localSet[item] += 1\n",
    "\n",
    "    for item, count in localSet.items():\n",
    "        support = float(count) / len(transactionList)\n",
    "\n",
    "        if support >= minSupport:\n",
    "            _itemSet.add(item)\n",
    "\n",
    "    return _itemSet\n",
    "\n",
    "\n",
    "def joinSet(itemSet, length):\n",
    "    \"\"\"Join a set with itself and returns the n-element itemsets\"\"\"\n",
    "    return set(\n",
    "        [i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length]\n",
    "    )\n",
    "\n",
    "\n",
    "def getItemSetTransactionList(data_iterator):\n",
    "    transactionList = list()\n",
    "    itemSet = set()\n",
    "    for record in data_iterator:\n",
    "        transaction = frozenset(record)\n",
    "        transactionList.append(transaction)\n",
    "        for item in transaction:\n",
    "            itemSet.add(frozenset([item]))  # Generate 1-itemSets\n",
    "    return itemSet, transactionList\n",
    "\n",
    "\n",
    "def runApriori(data_iter, minSupport, minConfidence):\n",
    "    \"\"\"\n",
    "    run the apriori algorithm. data_iter is a record iterator\n",
    "    Return both:\n",
    "     - items (tuple, support)\n",
    "     - rules ((pretuple, posttuple), confidence)\n",
    "    \"\"\"\n",
    "    itemSet, transactionList = getItemSetTransactionList(data_iter)\n",
    "\n",
    "    freqSet = defaultdict(int)\n",
    "    largeSet = dict()\n",
    "\n",
    "\n",
    "    assocRules = dict()\n",
    "\n",
    "    oneCSet = returnItemsWithMinSupport(itemSet, transactionList, minSupport, freqSet)\n",
    "\n",
    "    currentLSet = oneCSet\n",
    "    k = 2\n",
    "    while currentLSet != set([]):\n",
    "        largeSet[k - 1] = currentLSet\n",
    "        currentLSet = joinSet(currentLSet, k)\n",
    "        currentCSet = returnItemsWithMinSupport(\n",
    "            currentLSet, transactionList, minSupport, freqSet\n",
    "        )\n",
    "        currentLSet = currentCSet\n",
    "        k = k + 1\n",
    "\n",
    "    def getSupport(item):\n",
    "        \"\"\"local function which Returns the support of an item\"\"\"\n",
    "        return float(freqSet[item]) / len(transactionList)\n",
    "\n",
    "    toRetItems = []\n",
    "    for key, value in largeSet.items():\n",
    "        toRetItems.extend([(tuple(item), getSupport(item)) for item in value])\n",
    "\n",
    "    toRetRules = []\n",
    "    for key, value in list(largeSet.items())[1:]:\n",
    "        for item in value:\n",
    "            _subsets = map(frozenset, [x for x in subsets(item)])\n",
    "            for element in _subsets:\n",
    "                remain = item.difference(element)\n",
    "                if len(remain) > 0:\n",
    "                    confidence = getSupport(item) / getSupport(element)\n",
    "                    if confidence >= minConfidence:\n",
    "                        toRetRules.append(((tuple(element), tuple(remain)), confidence))\n",
    "    return toRetItems, toRetRules\n",
    "\n",
    "\n",
    "def printResults(items, rules):\n",
    "    \"\"\"prints the generated itemsets sorted by support and the confidence rules sorted by confidence\"\"\"\n",
    "    for item, support in sorted(items, key=lambda x: x[1]):\n",
    "        print(\"item: %s , %.3f\" % (str(item), support))\n",
    "    print(\"\\n------------------------ RULES:\")\n",
    "    for rule, confidence in sorted(rules, key=lambda x: x[1]):\n",
    "        pre, post = rule\n",
    "        print(\"Rule: %s ==> %s , %.3f\" % (str(pre), str(post), confidence))\n",
    "\n",
    "\n",
    "def to_str_results(items, rules):\n",
    "    \"\"\"prints the generated itemsets sorted by support and the confidence rules sorted by confidence\"\"\"\n",
    "    i, r = [], []\n",
    "    for item, support in sorted(items, key=lambda x: x[1]):\n",
    "        x = \"item: %s , %.3f\" % (str(item), support)\n",
    "        i.append(x)\n",
    "\n",
    "    for rule, confidence in sorted(rules, key=lambda x: x[1]):\n",
    "        pre, post = rule\n",
    "        x = \"Rule: %s ==> %s , %.3f\" % (str(pre), str(post), confidence)\n",
    "        r.append(x)\n",
    "\n",
    "    return i, r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the above code requires a csv in the form of \n",
    "\n",
    "Transaction 1 items\n",
    "\n",
    "Transaction 2 items \n",
    "... \n",
    "\n",
    "Modify the csv to appear as such "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_apriori(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        next(infile)\n",
    "        \n",
    "        for line in infile:\n",
    "            parts = line.strip().split(',')\n",
    "        \n",
    "            pois = [poi.strip('\"') for poi in parts[2:] if poi]  \n",
    "            pois_line = ','.join(pois)\n",
    "            outfile.write(f\"{pois_line}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataFromFile(fname):\n",
    "    \"\"\"Function which reads from the file and yields a generator\"\"\"\n",
    "    with open(fname, \"r\") as file_iter: \n",
    "        for line in file_iter:\n",
    "            line = line.strip().rstrip(\",\")  \n",
    "            record = frozenset(line.split(\",\"))  \n",
    "            yield record\n",
    "\n",
    "def run_apriori_and_save_results(input_file, min_support, min_confidence, dataset_label):\n",
    "    inFile = dataFromFile(input_file)\n",
    "    items, rules = runApriori(inFile, min_support, min_confidence)\n",
    "    \n",
    "\n",
    "    with open(f'output/itemsets_{dataset_label}.csv', 'w', newline='') as items_file:\n",
    "        writer = csv.writer(items_file)\n",
    "        writer.writerow(['Itemset', 'Support'])\n",
    "        for item, support in items:\n",
    "            writer.writerow([', '.join(item), support])\n",
    "\n",
    "    with open(f'output/rules_{dataset_label}.csv', 'w', newline='') as rules_file:\n",
    "        writer = csv.writer(rules_file)\n",
    "        writer.writerow(['Rule', 'Confidence'])\n",
    "        for (pre, post), confidence in rules:\n",
    "            rule = f\"{', '.join(pre)} => {', '.join(post)}\"\n",
    "            writer.writerow([rule, confidence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_files, min_support, min_confidence):\n",
    "    for input_file in input_files:\n",
    "        dataset_label = input_file.split('_')[-1].replace('.csv', '')  # Extract 'A', 'B', 'C', or 'D'\n",
    "        \n",
    "        transformed_file = f'data/transformed_{dataset_label}.csv'\n",
    "        preprocess_data(input_file, transformed_file)\n",
    "        \n",
    "        prepared_file = f'data/prepared_data_{dataset_label}.csv'\n",
    "        prepare_data_for_apriori(transformed_file, prepared_file)\n",
    "        \n",
    "        run_apriori_and_save_results(prepared_file, min_support, min_confidence, dataset_label)\n",
    "\n",
    "input_files = [\n",
    "    'data/POIdata_cityA.csv',\n",
    "    'data/POIdata_cityB.csv',\n",
    "    'data/POIdata_cityC.csv',\n",
    "    'data/POIdata_cityD.csv'\n",
    "]\n",
    "min_support = 0.10\n",
    "min_confidence = 0.3\n",
    "\n",
    "main(input_files, min_support, min_confidence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
